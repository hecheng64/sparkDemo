一环境搭建win10环境
1.kafka环境搭建
参数win10kafka环境搭建和kafka-manager环境搭建使用
https://www.cnblogs.com/dadonggg/p/8205302.html
https://blog.csdn.net/csgarten/article/details/80463453

下载
zookeeper-3.4.13
kafka_2.12-2.0.0
kafka-manager-1.3.3.17
百度网盘地址：

配置zookeeper环境变量

启动 kafka_2.12-2.0.0下
startup-zk.bat  //启动zookeeper
startup-kafka.bat //启动kafka

启动kafka-manager-1.3.3.17下
manager-startup.bat //启动kafka manager 

zookeeper 端口配置2181
kafka端口broker监听端口9092(可以从kafka-manager的brokers列表查看)
kafka-manager 服务地址 http://localhost:9000/

2.spark环境搭建
下载
hadoop-2.6.4
scala-2.11.8.msi
spark-2.1.0-bin-hadoop2.6
百度网盘地址：

hadoop环境变量配置
scala环境变量配置

启动spark-2.1.0-bin-hadoop2.6/bin下
spark-shell_start.bat //启动spark，依赖启动hadoop

spark启动后web管理地址 http://127.0.0.1:4040/jobs/

spark.master 地址 local[*] (可以在web管理界面看Environment -》Spark Properties找到配置)


二.工程搭建
1.新建springboot工程

2.pom.xml 包依赖如下
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
   <modelVersion>4.0.0</modelVersion>

   <groupId>com.cz.example</groupId>
   <artifactId>sparkDemo</artifactId>
   <version>0.0.1-SNAPSHOT</version>
   <packaging>jar</packaging>

   <name>sparkDemo</name>
   <description>Demo project for Spring Boot</description>

   <parent>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-parent</artifactId>
      <version>1.5.15.RELEASE</version>
      <relativePath/> <!-- lookup parent from repository -->
   </parent>

   <properties>
      <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
      <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
      <java.version>1.8</java.version>
      <scala.version>2.11</scala.version>
      <spark.version>2.1.0</spark.version>
      <springboot.version>1.5.15.RELEASE</springboot.version>
   </properties>

   <dependencies>
      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-starter-web</artifactId>
      </dependency>


      <!--<dependency>-->
         <!--<groupId>org.springframework.boot</groupId>-->
         <!--<artifactId>spring-boot-starter</artifactId>-->
         <!--<version>${springboot.version}</version>-->
         <!--<exclusions>-->
            <!--<exclusion>-->
               <!--<groupId>org.springframework.boot</groupId>-->
               <!--<artifactId>spring-boot-starter-logging</artifactId>-->
            <!--</exclusion>-->
         <!--</exclusions>-->
      <!--</dependency>-->
      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-starter-log4j2</artifactId>
         <version>${springboot.version}</version>
      </dependency>

      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-starter-aop</artifactId>
      </dependency>

      <dependency>
         <groupId>org.apache.spark</groupId>
         <artifactId>spark-core_${scala.version}</artifactId>
         <version>${spark.version}</version>
         <exclusions>
            <exclusion>
               <groupId>org.slf4j</groupId>
               <artifactId>slf4j-log4j12</artifactId>
            </exclusion>
            <exclusion>
               <groupId>log4j</groupId>
               <artifactId>log4j</artifactId>
            </exclusion>
         </exclusions>
         <scope>compile</scope>
      </dependency>

      <dependency>
         <groupId>org.apache.spark</groupId>
         <artifactId>spark-streaming_${scala.version}</artifactId>
         <version>${spark.version}</version>
         <scope>compile</scope>
      </dependency>

      <dependency>
         <groupId>org.apache.spark</groupId>
         <artifactId>spark-sql_${scala.version}</artifactId>
         <version>${spark.version}</version>
      </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-8_${scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>compile</scope>
        </dependency>

      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-starter-test</artifactId>
         <scope>test</scope>
      </dependency>

      <dependency>
         <groupId>org.springframework.boot</groupId>
         <artifactId>spring-boot-configuration-processor</artifactId>
         <optional>true</optional>
      </dependency>
   </dependencies>

   <build>
      <plugins>
         <plugin>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
         </plugin>
      </plugins>
   </build>

   <repositories>
      <repository>
         <id>spring-snapshots</id>
         <name>Spring Snapshots</name>
         <url>https://repo.spring.io/snapshot</url>
         <snapshots>
            <enabled>true</enabled>
         </snapshots>
      </repository>
      <repository>
         <id>spring-milestones</id>
         <name>Spring Milestones</name>
         <url>https://repo.spring.io/milestone</url>
         <snapshots>
            <enabled>false</enabled>
         </snapshots>
      </repository>
   </repositories>

   <pluginRepositories>
      <pluginRepository>
         <id>spring-snapshots</id>
         <name>Spring Snapshots</name>
         <url>https://repo.spring.io/snapshot</url>
         <snapshots>
            <enabled>true</enabled>
         </snapshots>
      </pluginRepository>
      <pluginRepository>
         <id>spring-milestones</id>
         <name>Spring Milestones</name>
         <url>https://repo.spring.io/milestone</url>
         <snapshots>
            <enabled>false</enabled>
         </snapshots>
      </pluginRepository>
   </pluginRepositories>


</project>

3.配置文件application.properties配置
server.port=8054

spark.spark-home=.
spark.app-name=sparkTest
#spark master 地址 默认端口7077
spark.master=local[*]


#spark topics ','号分割
spark.kafka.topics = news
#kafka集群地址，'，'号分割
kafka.broker.list =127.0.0.1:9092
#从kafka拉数据的间隔时间，单位 S
spark.stream.kafka.durations=10


4.创建kafka数据生成者

package com.cz.example.sparkDemo.config;

import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.core.annotation.Order;
import org.springframework.stereotype.Component;

import java.util.Properties;
import java.util.concurrent.TimeUnit;

import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
import kafka.serializer.StringEncoder;

/**
 *
 * 启动kafka数据产生
 * @author
 *
 */
@Component
public class KafkaProduct extends Thread {


    @Value("${kafka.broker.list}")
    private String metadatabrokerlist;
    @Value("${spark.kafka.topics}")
    private String topic;

    public KafkaProduct(){
        super();
    }

    @Override
    public void run() {
        Producer producer = createProducer();
        int i=0;
        while(true){
            producer.send(new KeyedMessage<Integer, String>(topic, "message: " + i++));
            try {
                TimeUnit.SECONDS.sleep(1);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

    private Producer createProducer() {
        Properties properties = new Properties();
        properties.put("zookeeper.connect", "localhost:2181");//声明zk
        properties.put("serializer.class", StringEncoder.class.getName());
        properties.put("metadata.broker.list", "localhost:9092");// 声明kafka broker
        return new Producer<Integer, String>(new ProducerConfig(properties));
    }
}

5.sparkStream实时流消费
package com.cz.example.sparkDemo.service.impl;

import kafka.serializer.StringDecoder;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.io.Serializable;
import java.util.*;

@Component
public class SparkKafkaStreamExecutor implements Serializable,Runnable{
    /**
    * 
    */
   private static final long serialVersionUID = 1L;
   private static final Logger log = LoggerFactory.getLogger(SparkKafkaStreamExecutor.class);
   
   @Value("${spark.stream.kafka.durations}")
   private String streamDurationTime;
   @Value("${kafka.broker.list}")
   private String metadatabrokerlist;
   @Value("${spark.kafka.topics}")
   private String topicsAll;
// @Autowired
// private transient Gson gson;

   private transient JavaStreamingContext jsc;
   @Autowired 
   private transient JavaSparkContext javaSparkContext;
   
   @Override
   public void run() {
      startStreamTask();
   }
   
   public void startStreamTask() {
      Set<String> topics = new HashSet<String>(Arrays.asList(topicsAll.split(",")));

      Map<String, String> kafkaParams = new HashMap<>();
      kafkaParams.put("metadata.broker.list", metadatabrokerlist);
      
      jsc = new JavaStreamingContext(javaSparkContext,
            Durations.seconds(Integer.valueOf(streamDurationTime)));
      jsc.checkpoint("checkpoint"); //保证元数据恢复，就是Driver端挂了之后数据仍然可以恢复

      // 得到数据流
      final JavaPairInputDStream<String, String> stream = KafkaUtils.createDirectStream(jsc, String.class,
            String.class, StringDecoder.class, StringDecoder.class, kafkaParams, topics);
      System.out.println("stream started!");
      stream.print();
      stream.foreachRDD(v -> {
         //针对单篇文章流式处理
         List<String> topicDatas = v.values().collect();//优化点：不收集而是并发节点处理？
         topicDatas.parallelStream().forEach(m ->{
            System.out.println(m);
         });
         log.info("一批次数据流处理完： {}",topicDatas);
      });
      //优化：为每个分区创建一个连接
//    stream.foreachRDD(t->{
//       t.foreachPartition(f->{
//          while(f.hasNext()) {
//             Map<String, Object> symbolLDAHandlered =LDAModelPpl
//                   .LDAHandlerOneArticle(sparkSession, SymbolAndNews.symbolHandlerOneArticle(sparkSession, f.next()._2));
//          }
//       });
//    });
      jsc.start();
   }

   public void destoryStreamTask() {
      if(jsc!=null) {
         jsc.stop();
      }     
   }

}

6.服务启动后加载数据生产消费
package com.cz.example.sparkDemo.config;

import com.cz.example.sparkDemo.service.impl.SparkKafkaStreamExecutor;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.CommandLineRunner;
import org.springframework.core.annotation.Order;
import org.springframework.stereotype.Component;

/**
 * spring boot 容器加载完成后执行
 * 启动kafka数据接收和处理
 *
 * @author
 */
@Component
@Order(value = 2)
public class SparkStartup implements CommandLineRunner {

    @Autowired
    SparkKafkaStreamExecutor sparkKafkaStreamExecutor;
    @Autowired
    KafkaProduct kafkaProduct;

    @Override
    public void run(String... args) throws Exception {
        //启动数据生产者
        new Thread(kafkaProduct).start();

        //启动消费者
        Thread thread = new Thread(sparkKafkaStreamExecutor);
        thread.start();
    }

}

7.spark配置加载
package com.cz.example.sparkDemo.config;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
@ConfigurationProperties(prefix = "spark")
public class SparkConfig {

    private String sparkHome = ".";

    private String appName = "sparkTest";

    private String master = "local";

    @Bean
    @ConditionalOnMissingBean(SparkConf.class)
    public SparkConf sparkConf() throws Exception {
        SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
        return conf;
    }

    @Bean
    @ConditionalOnMissingBean(JavaSparkContext.class)
    public JavaSparkContext javaSparkContext() throws Exception {
        return new JavaSparkContext(sparkConf());
    }

    public String getSparkHome() {
        return sparkHome;
    }

    public void setSparkHome(String sparkHome) {
        this.sparkHome = sparkHome;
    }

    public String getAppName() {
        return appName;
    }

    public void setAppName(String appName) {
        this.appName = appName;
    }

    public String getMaster() {
        return master;
    }

    public void setMaster(String master) {
        this.master = master;
    }
}


RabbitMq数据流处理参考自定义接收stream流进行改造：
通过rabbitmq数据接收监听数据存入临时queue队列，自定义接收stream流从queue中取数据

源代码git地址：